{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos usando la función read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = \"/JupyterNotebooks/machine_learning/datasets/\"\n",
    "filename = \"titanic/titanic3.csv\"\n",
    "fullpath = os.path.join(mainpath,filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argumentos de la funcion read_csv \n",
    "* sep : separador de columnas, puede ser una Exp Reg\n",
    "* dtype: especificar el tipo de datos de cada columna, dtype=(\"a\":np.float64); la col \"a\"  se carga como float\n",
    "* header: para especificar que fila es la cabecera del dataset, por defecto se usa la 1era fila\n",
    "* name: array o lita para nombrar o renombrar las columnas de un dataset\n",
    "* skiprows: para saltar a una fila especifica\n",
    "* index_col  indicar una columna especifica como identificador\n",
    "* skip_blank_lines : para saltarse valores en blanco o NaN\n",
    "* nan_filter: elimina cualquier fila que tenga valores NaN o strings vacios, mucho cuidado con esta!!\n",
    "\n",
    "  read.csv(**filepath**=\"/JupyterNotebooks/machine_learning/datasets/titanic/titanic3.csv\", **sep**=\",\", **dtype**=None, **header**=0, **names**={\"nameA\",\"nameB\"}, **skiprows**=12, **index_col**=None, **skip_blank_lines**=True, **na_filte**=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener de dos archivos distintos las cabeceras y las filas de un dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtengo un dataset .txt, read_csv() lo lee porque ya esta separado por comas\n",
    "data2 = pd.read_csv(mainpath+\"/\"+\"customer-churn-model/Customer Churn Model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se verifica que el dataset cargo corectamente y vemos los nombres originales de sus cabeceras\n",
    "data2.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esta es la lista de cabeceras originales del dataset\n",
    "data2.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtengo otro grupo de cabeceras desde otro archivo y las transformo en una lista\n",
    "data_cols = pd.read_csv(mainpath + \"/\" +\"customer-churn-model/Customer Churn Columns.csv\")\n",
    "data_col_list = data_cols[\"Column_Names\"].tolist()\n",
    "data_col_list\n",
    "\n",
    "#ahora uso esa lista para sustituir las cabeceras originales del dataset\n",
    "data2 = pd.read_csv(mainpath+\"/\"+\"customer-churn-model/Customer Churn Model.txt\",\n",
    "                    header = None, names = data_col_list)\n",
    "data2.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de grandes archivos con la funcion OPEN\n",
    "extrae los datos del archivo linea a linea, y se pude ir almacenando, de forma manual con distintas operaciones para obtener el mismo resultado que con read_csv() pero en archivos de gran tamaño. Hay que tener cuidado al abrir un fichero en modo \"w\" (escritura) ya que esto hará que dicho fichero pierda su contenido (en caso de tenerlo y querer conservarlo). Si por algún motivo se quiere agregar contenido a este es recomendable abrirlo en modo \"a\" (append), lo cual posicionará el cursor al final y no borrará el contenido del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##abrimos en modo solo lectura \"r\"\n",
    "data3 = open(mainpath + \"/\"+\"customer-churn-model/Customer Churn Model.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next() va a la siguiente fila del archivo,que sigue de la cabecera,\n",
    "#strip, elimina espacios en blanco al inicio y al final de la fila\n",
    "#split() divide usando la , asi obtenemos las columnas del dataset\n",
    "cols = data3.readline().strip().split(\",\")\n",
    "n_cols = len(cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crea un diccionario, se parte por las cabeceras extraidas en cols y por cada cabecera un array vacio que \n",
    "#luego se rellenara con los datos, quedando una estructura estilo JSON\n",
    "counter = 0\n",
    "main_dict ={}\n",
    "for col in cols:\n",
    "    main_dict[col] =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rellenar el diccionario leyendo cada linea del dataset (line), esquema de lectura y llenado de matriz NxN\n",
    "for line in data3:\n",
    "    #cada valor se obtendra de cada linea del dataset quitando espacios vacios y dividido por comas, como antes\n",
    "    values = line.strip().split(\",\") \n",
    "    #cada [] del diccionario sera un array con los valores leidos y procesados desde el dataset\n",
    "    for i in range(len(cols)):\n",
    "        main_dict[cols[i]].append(values[i])\n",
    "    counter +=1\n",
    "print(\"El dataset tiene %d filas y %d columnas\" %(counter,n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos un dataframe de panda y el resultado es el mismo que con la funcion read_csv, recordar que esto es para grandes\n",
    "#datasets que podrian ser cargados en partes, o incluso en distintas maquinas y hacer la carga manual de datos\n",
    "df3 = pd.DataFrame(main_dict)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura y escritura de archivos \n",
    "usaremos el archivo anterior para crear un nuevo archivo igual pero que usa el tabulador \"/t\" como separador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile =  mainpath + \"/\" +\"customer-churn-model/Customer Churn Model.txt\"\n",
    "outfile = mainpath + \"/\" +\"customer-churn-model/Tab Customer Churn Model.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(infile,\"r\") as infile1:\n",
    "    with open(outfile,\"w\") as outfile1:\n",
    "        for line in infile1:\n",
    "            fields = line.strip().split(\",\")#lee cada linea del archivo original separada por comas\n",
    "            outfile1.write(\"\\t\".join(fields))#escribe cada linea anterior en el nuevo archivo, separadas por tabulador\n",
    "            outfile1.write(\"\\n\") #intro para pasar de linea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si usamos read_csv indicando el separador \"\\t\" el resultado sigue siendo el mismo\n",
    "df4 = pd.read_csv(outfile,sep=\"\\t\")\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer datos desde una url externa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usando pandas\n",
    "medals_url = \"http://winterolympicsmedals.com/medals.csv\"\n",
    "medals_data = pd.read_csv(medals_url)\n",
    "medals_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer archivos XLS y XSLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainpath = \"/JupyterNotebooks/machine_learning/datasets/\"\n",
    "filename = \"titanic/titanic3.xls\"\n",
    "titanic2 = pd.read_excel(mainpath+\"/\"+filename,\"titanic3\")\n",
    "titanic2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creo un archivo .csv a partir de un dataframe, y tambien se puede crear un excel o csv o json\n",
    "titanic2.to_csv(mainpath + \"/titanic/titanic_custom2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.to_excel(mainpath + \"/titanic/titanic_custom3.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.to_json(mainpath + \"/titanic/titanic_custom4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
